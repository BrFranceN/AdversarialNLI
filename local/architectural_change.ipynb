{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Parameters\n",
    "# we will use with Distil-BERT\n",
    "language_model_name = \"distilbert-base-uncased\"\n",
    "language_model_name2 = \"bert-base-uncased\"\n",
    "\n",
    "### Training Argurments\n",
    "# this GPU should be enough for this task to handle 32 samples per batch\n",
    "batch_size = 32\n",
    "\n",
    "# optim\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.001 # we could use e.g. 0.01 in case of very low and very high amount of data for regularization\n",
    "\n",
    "# training\n",
    "epochs = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(42)\n",
    "nli_dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\",trust_remote_code=True)\n",
    "adversarial_set = load_dataset(\"iperbole/adversarial_fever_nli\")\n",
    "\n",
    "\n",
    "\n",
    "### METRIC DEFINITION\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "    load_precision = load_metric(\"precision\")\n",
    "    load_recall = load_metric(\"recall\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    precision = load_precision.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall = load_recall.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model,'model1.pth')\n",
    "# torch.save(model2,'model3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(model1)\n",
    "# del(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3920e1638d4b8785f575d18163db28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '150448', 'premise': \"Roman Atwood . He is best known for his vlogs , where he posts updates about his life on a daily basis . His vlogging channel , `` RomanAtwoodVlogs '' , has a total of 3.3 billion views and 11.9 million subscribers . He also has another YouTube channel called `` RomanAtwood '' , where he posts pranks .\", 'hypothesis': 'Roman Atwood is a content creator.', 'label': 0, 'wsd': {'premise': [{'index': 0, 'text': 'Roman', 'pos': 'ADJ', 'lemma': 'roman', 'bnSynsetId': 'bn:00109913a', 'wnSynsetOffset': '2921569a', 'nltkSynset': 'roman.a.01'}, {'index': 1, 'text': 'Atwood', 'pos': 'PROPN', 'lemma': 'Atwood', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 2, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 3, 'text': 'He', 'pos': 'PRON', 'lemma': 'he', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 4, 'text': 'is', 'pos': 'AUX', 'lemma': 'be', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 5, 'text': 'best', 'pos': 'ADV', 'lemma': 'well', 'bnSynsetId': 'bn:00117603r', 'wnSynsetOffset': '12779r', 'nltkSynset': 'well.r.02'}, {'index': 6, 'text': 'known', 'pos': 'VERB', 'lemma': 'know', 'bnSynsetId': 'bn:00090143v', 'wnSynsetOffset': '594337v', 'nltkSynset': 'know.v.04'}, {'index': 7, 'text': 'for', 'pos': 'ADP', 'lemma': 'for', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 8, 'text': 'his', 'pos': 'PRON', 'lemma': 'his', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 9, 'text': 'vlogs', 'pos': 'NOUN', 'lemma': 'vlog', 'bnSynsetId': 'bn:00028604n', 'wnSynsetOffset': '7007945n', 'nltkSynset': 'play.n.01'}, {'index': 10, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 11, 'text': 'where', 'pos': 'SCONJ', 'lemma': 'where', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 12, 'text': 'he', 'pos': 'PRON', 'lemma': 'he', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 13, 'text': 'posts', 'pos': 'VERB', 'lemma': 'post', 'bnSynsetId': 'bn:00091876v', 'wnSynsetOffset': '991683v', 'nltkSynset': 'post.v.02'}, {'index': 14, 'text': 'updates', 'pos': 'NOUN', 'lemma': 'update', 'bnSynsetId': 'bn:00079238n', 'wnSynsetOffset': '6643303n', 'nltkSynset': 'update.n.01'}, {'index': 15, 'text': 'about', 'pos': 'ADP', 'lemma': 'about', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 16, 'text': 'his', 'pos': 'PRON', 'lemma': 'his', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 17, 'text': 'life', 'pos': 'NOUN', 'lemma': 'life', 'bnSynsetId': 'bn:00051045n', 'wnSynsetOffset': '13963192n', 'nltkSynset': 'life.n.01'}, {'index': 18, 'text': 'on', 'pos': 'ADP', 'lemma': 'on', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 19, 'text': 'a', 'pos': 'DET', 'lemma': 'a', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 20, 'text': 'daily', 'pos': 'ADJ', 'lemma': 'daily', 'bnSynsetId': 'bn:00100875a', 'wnSynsetOffset': '1968165a', 'nltkSynset': 'daily.s.01'}, {'index': 21, 'text': 'basis', 'pos': 'NOUN', 'lemma': 'basis', 'bnSynsetId': 'bn:00008870n', 'wnSynsetOffset': '13790912n', 'nltkSynset': 'footing.n.02'}, {'index': 22, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 23, 'text': 'His', 'pos': 'PRON', 'lemma': 'his', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 24, 'text': 'vlogging', 'pos': 'VERB', 'lemma': 'vlogge', 'bnSynsetId': 'bn:00090000v', 'wnSynsetOffset': '672277v', 'nltkSynset': 'judge.v.01'}, {'index': 25, 'text': 'channel', 'pos': 'NOUN', 'lemma': 'channel', 'bnSynsetId': 'bn:00017686n', 'wnSynsetOffset': '3006398n', 'nltkSynset': 'channel.n.07'}, {'index': 26, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 27, 'text': '`', 'pos': 'PUNCT', 'lemma': '`', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 28, 'text': '`', 'pos': 'PUNCT', 'lemma': '`', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 29, 'text': 'RomanAtwoodVlogs', 'pos': 'X', 'lemma': 'romanatwoodvlogs', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 30, 'text': \"''\", 'pos': 'PUNCT', 'lemma': \"''\", 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 31, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 32, 'text': 'has', 'pos': 'AUX', 'lemma': 'have', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 33, 'text': 'a', 'pos': 'DET', 'lemma': 'a', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 34, 'text': 'total', 'pos': 'NOUN', 'lemma': 'total', 'bnSynsetId': 'bn:00002008n', 'wnSynsetOffset': '4353803n', 'nltkSynset': 'sum.n.05'}, {'index': 35, 'text': 'of', 'pos': 'ADP', 'lemma': 'of', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 36, 'text': '3.3', 'pos': 'NUM', 'lemma': '3.3', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 37, 'text': 'billion', 'pos': 'NUM', 'lemma': 'billion', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 38, 'text': 'views', 'pos': 'NOUN', 'lemma': 'view', 'bnSynsetId': 'bn:00071511n', 'wnSynsetOffset': '881649n', 'nltkSynset': 'view.n.03'}, {'index': 39, 'text': 'and', 'pos': 'CCONJ', 'lemma': 'and', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 40, 'text': '11.9', 'pos': 'NUM', 'lemma': '11.9', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 41, 'text': 'million', 'pos': 'NUM', 'lemma': 'million', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 42, 'text': 'subscribers', 'pos': 'NOUN', 'lemma': 'subscriber', 'bnSynsetId': 'bn:00066366n', 'wnSynsetOffset': '10670483n', 'nltkSynset': 'subscriber.n.02'}, {'index': 43, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 44, 'text': 'He', 'pos': 'PRON', 'lemma': 'he', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 45, 'text': 'also', 'pos': 'ADV', 'lemma': 'also', 'bnSynsetId': 'bn:00114246r', 'wnSynsetOffset': '47534r', 'nltkSynset': 'besides.r.02'}, {'index': 46, 'text': 'has', 'pos': 'AUX', 'lemma': 'have', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 47, 'text': 'another', 'pos': 'DET', 'lemma': 'another', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 48, 'text': 'YouTube', 'pos': 'PROPN', 'lemma': 'YouTube', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 49, 'text': 'channel', 'pos': 'NOUN', 'lemma': 'channel', 'bnSynsetId': 'bn:00015144n', 'wnSynsetOffset': '5250659n', 'nltkSynset': 'duct.n.01'}, {'index': 50, 'text': 'called', 'pos': 'VERB', 'lemma': 'call', 'bnSynsetId': 'bn:00084385v', 'wnSynsetOffset': '1028748v', 'nltkSynset': 'name.v.01'}, {'index': 51, 'text': '`', 'pos': 'PUNCT', 'lemma': '`', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 52, 'text': '`', 'pos': 'PUNCT', 'lemma': '`', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 53, 'text': 'RomanAtwood', 'pos': 'PROPN', 'lemma': 'RomanAtwood', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 54, 'text': \"''\", 'pos': 'PUNCT', 'lemma': \"''\", 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 55, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 56, 'text': 'where', 'pos': 'SCONJ', 'lemma': 'where', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 57, 'text': 'he', 'pos': 'PRON', 'lemma': 'he', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 58, 'text': 'posts', 'pos': 'VERB', 'lemma': 'post', 'bnSynsetId': 'bn:00091876v', 'wnSynsetOffset': '991683v', 'nltkSynset': 'post.v.02'}, {'index': 59, 'text': 'pranks', 'pos': 'NOUN', 'lemma': 'prank', 'bnSynsetId': 'bn:00004630n', 'wnSynsetOffset': '427580n', 'nltkSynset': 'antic.n.01'}, {'index': 60, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}], 'hypothesis': [{'index': 0, 'text': 'Roman', 'pos': 'PROPN', 'lemma': 'Roman', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 1, 'text': 'Atwood', 'pos': 'PROPN', 'lemma': 'Atwood', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 2, 'text': 'is', 'pos': 'AUX', 'lemma': 'be', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 3, 'text': 'a', 'pos': 'DET', 'lemma': 'a', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 4, 'text': 'content', 'pos': 'ADJ', 'lemma': 'content', 'bnSynsetId': 'bn:00100364a', 'wnSynsetOffset': '588797a', 'nltkSynset': 'contented.a.01'}, {'index': 5, 'text': 'creator', 'pos': 'NOUN', 'lemma': 'creator', 'bnSynsetId': 'bn:00023660n', 'wnSynsetOffset': '9614315n', 'nltkSynset': 'creator.n.02'}, {'index': 6, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}]}, 'srl': {'premise': {'tokens': [{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': '.'}, {'index': 3, 'rawText': 'He'}, {'index': 4, 'rawText': 'is'}, {'index': 5, 'rawText': 'best'}, {'index': 6, 'rawText': 'known'}, {'index': 7, 'rawText': 'for'}, {'index': 8, 'rawText': 'his'}, {'index': 9, 'rawText': 'vlogs'}, {'index': 10, 'rawText': ','}, {'index': 11, 'rawText': 'where'}, {'index': 12, 'rawText': 'he'}, {'index': 13, 'rawText': 'posts'}, {'index': 14, 'rawText': 'updates'}, {'index': 15, 'rawText': 'about'}, {'index': 16, 'rawText': 'his'}, {'index': 17, 'rawText': 'life'}, {'index': 18, 'rawText': 'on'}, {'index': 19, 'rawText': 'a'}, {'index': 20, 'rawText': 'daily'}, {'index': 21, 'rawText': 'basis'}, {'index': 22, 'rawText': '.'}, {'index': 23, 'rawText': 'His'}, {'index': 24, 'rawText': 'vlogging'}, {'index': 25, 'rawText': 'channel'}, {'index': 26, 'rawText': ','}, {'index': 27, 'rawText': '`'}, {'index': 28, 'rawText': '`'}, {'index': 29, 'rawText': 'RomanAtwoodVlogs'}, {'index': 30, 'rawText': \"''\"}, {'index': 31, 'rawText': ','}, {'index': 32, 'rawText': 'has'}, {'index': 33, 'rawText': 'a'}, {'index': 34, 'rawText': 'total'}, {'index': 35, 'rawText': 'of'}, {'index': 36, 'rawText': '3.3'}, {'index': 37, 'rawText': 'billion'}, {'index': 38, 'rawText': 'views'}, {'index': 39, 'rawText': 'and'}, {'index': 40, 'rawText': '11.9'}, {'index': 41, 'rawText': 'million'}, {'index': 42, 'rawText': 'subscribers'}, {'index': 43, 'rawText': '.'}, {'index': 44, 'rawText': 'He'}, {'index': 45, 'rawText': 'also'}, {'index': 46, 'rawText': 'has'}, {'index': 47, 'rawText': 'another'}, {'index': 48, 'rawText': 'YouTube'}, {'index': 49, 'rawText': 'channel'}, {'index': 50, 'rawText': 'called'}, {'index': 51, 'rawText': '`'}, {'index': 52, 'rawText': '`'}, {'index': 53, 'rawText': 'RomanAtwood'}, {'index': 54, 'rawText': \"''\"}, {'index': 55, 'rawText': ','}, {'index': 56, 'rawText': 'where'}, {'index': 57, 'rawText': 'he'}, {'index': 58, 'rawText': 'posts'}, {'index': 59, 'rawText': 'pranks'}, {'index': 60, 'rawText': '.'}], 'annotations': [{'tokenIndex': 4, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 22]}]}, 'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARG2', 'score': 1.0, 'span': [5, 22]}]}}, {'tokenIndex': 6, 'verbatlas': {'frameName': 'KNOW', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 6]}, {'role': 'Topic', 'score': 1.0, 'span': [7, 22]}]}, 'englishPropbank': {'frameName': 'know.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARGM-MNR', 'score': 1.0, 'span': [5, 6]}, {'role': 'ARG2', 'score': 1.0, 'span': [7, 22]}]}}, {'tokenIndex': 13, 'verbatlas': {'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [8, 10]}, {'role': 'Agent', 'score': 1.0, 'span': [12, 13]}, {'role': 'Theme', 'score': 1.0, 'span': [14, 18]}, {'role': 'Time', 'score': 1.0, 'span': [18, 22]}]}, 'englishPropbank': {'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [8, 10]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [11, 12]}, {'role': 'ARG0', 'score': 1.0, 'span': [12, 13]}, {'role': 'ARG1', 'score': 1.0, 'span': [14, 18]}, {'role': 'ARGM-TMP', 'score': 1.0, 'span': [18, 22]}]}}, {'tokenIndex': 32, 'verbatlas': {'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [23, 32]}, {'role': 'Attribute', 'score': 1.0, 'span': [33, 43]}]}, 'englishPropbank': {'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [23, 32]}, {'role': 'ARG1', 'score': 1.0, 'span': [33, 43]}]}}, {'tokenIndex': 46, 'verbatlas': {'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [44, 45]}, {'role': 'Attribute', 'score': 1.0, 'span': [47, 60]}]}, 'englishPropbank': {'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [44, 45]}, {'role': 'ARGM-DIS', 'score': 1.0, 'span': [45, 46]}, {'role': 'ARG1', 'score': 1.0, 'span': [47, 60]}]}}, {'tokenIndex': 50, 'verbatlas': {'frameName': 'NAME', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [47, 50]}, {'role': 'Attribute', 'score': 1.0, 'span': [51, 55]}]}, 'englishPropbank': {'frameName': 'call.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [47, 50]}, {'role': 'ARG2', 'score': 1.0, 'span': [51, 55]}]}}, {'tokenIndex': 58, 'verbatlas': {'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [47, 55]}, {'role': 'Agent', 'score': 1.0, 'span': [57, 58]}, {'role': 'Theme', 'score': 1.0, 'span': [59, 60]}]}, 'englishPropbank': {'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [47, 55]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [56, 57]}, {'role': 'ARG0', 'score': 1.0, 'span': [57, 58]}, {'role': 'ARG1', 'score': 1.0, 'span': [59, 60]}]}}]}, 'hypothesis': {'tokens': [{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': 'is'}, {'index': 3, 'rawText': 'a'}, {'index': 4, 'rawText': 'content'}, {'index': 5, 'rawText': 'creator'}, {'index': 6, 'rawText': '.'}], 'annotations': [{'tokenIndex': 2, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]}, {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]}, 'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [0, 2]}, {'role': 'ARG2', 'score': 1.0, 'span': [3, 6]}]}}]}}, 'input_ids': [101, 3142, 2012, 3702, 1012, 2002, 2003, 2190, 2124, 2005, 2010, 1058, 21197, 2015, 1010, 2073, 2002, 8466, 14409, 2055, 2010, 2166, 2006, 1037, 3679, 3978, 1012, 2010, 1058, 21197, 4726, 3149, 1010, 1036, 1036, 3142, 4017, 3702, 2615, 21197, 2015, 1005, 1005, 1010, 2038, 1037, 2561, 1997, 1017, 1012, 1017, 4551, 5328, 1998, 2340, 1012, 1023, 2454, 17073, 1012, 2002, 2036, 2038, 2178, 7858, 3149, 2170, 1036, 1036, 3142, 4017, 3702, 1005, 1005, 1010, 2073, 2002, 8466, 26418, 2015, 1012, 102, 3142, 2012, 3702, 2003, 1037, 4180, 8543, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "## Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(language_model_name,\n",
    "                                                                   ignore_mismatched_sizes=True,\n",
    "                                                                   output_attentions=False, output_hidden_states=False,\n",
    "                                                                   num_labels=3) # number of the classes to change to 3\n",
    "\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(language_model_name2,\n",
    "                                                                   ignore_mismatched_sizes=True,\n",
    "                                                                   output_attentions=False, output_hidden_states=False,\n",
    "                                                                   num_labels=3) # number of the classes to change to 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(language_model_name2)\n",
    "\n",
    "# padding with the most long sentence!\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # avoid to use can reduce the memory on GPU\n",
    "data_collator2 = DataCollatorWithPadding(tokenizer=tokenizer2) # avoid to use can reduce the memory on GPU\n",
    "\n",
    "#examples are batch!\n",
    "def tokenize_function(examples):\n",
    "    examples[\"label\"] = [labels_mapping[label] for label in examples[\"label\"]]\n",
    "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"],padding = True, truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the dataset ...\n",
    "print(\"Tokenize the dataset ...\")\n",
    "labels_mapping = {\"ENTAILMENT\":0, \"CONTRADICTION\":1, \"NEUTRAL\":2 }\n",
    "tokenized_datasets_nli = nli_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets_nli_2 = nli_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_adversarial_dataset = adversarial_set.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_datasets_nli[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e8b1f95a85440ba4b2a7c1e30870eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bitfra/miniconda3/envs/nlp_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 37\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train DistilBERT\u001b[39;00m\n\u001b[1;32m     28\u001b[0m trainer1 \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Train BERT\u001b[39;00m\n\u001b[1;32m     40\u001b[0m trainer3 \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     41\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel2,\n\u001b[1;32m     42\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     48\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.11/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#MODELS TRAINING\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_dir\",                    # output directory [Mandatory]\n",
    "    num_train_epochs=epochs,                      # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,       # batch size per device during training\n",
    "    warmup_steps=500,                             # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=weight_decay,                    # strength of weight decay\n",
    "    save_strategy=\"no\",                           # save the model\n",
    "    learning_rate=learning_rate,                  # learning rate\n",
    "    gradient_checkpointing = True                 # to reduce memory usage\n",
    "    # fp16 = True                                 # to reduce more memory usage\n",
    ")\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"training_dir\",                    # output directory [Mandatory]\n",
    "    num_train_epochs=epochs,                      # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,       # batch size per device during training\n",
    "    warmup_steps=500,                             # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=weight_decay,                    # strength of weight decay\n",
    "    save_strategy=\"no\",                           # save the model\n",
    "    learning_rate=learning_rate,                  # learning rate\n",
    "    gradient_checkpointing = True,              # to reduce memory usage\n",
    "    # fp16 = False                                 # to reduce more memory usage\n",
    ")\n",
    "\n",
    "# Train DistilBERT\n",
    "trainer1 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_nli[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_nli[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer1.train()\n",
    "\n",
    "# Train BERT\n",
    "trainer3 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=tokenized_datasets_nli_2[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_nli_2[\"validation\"],\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator2,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer3.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f19f7a1830429a92c5ea4ebab27142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15ee0a7478c436889938b889ddeaf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions from DistilBERT\n",
    "# predictions1 = trainer1.predict(tokenized_datasets_nli[\"validation\"]).predictions\n",
    "# probs1 = torch.nn.functional.softmax(torch.tensor(predictions1), dim=-1).numpy()\n",
    "\n",
    "\n",
    "# # Get predictions from BERT\n",
    "# predictions3 = trainer3.predict(tokenized_datasets_nli_3[\"validation\"]).predictions\n",
    "# probs3 = torch.nn.functional.softmax(torch.tensor(predictions3), dim=-1).numpy()\n",
    "\n",
    "\n",
    "# Get predictions from DistilBERT\n",
    "predictions1 = trainer1.predict(tokenized_adversarial_dataset[\"test\"]).predictions\n",
    "probs1 = torch.nn.functional.softmax(torch.tensor(predictions1), dim=-1).numpy()\n",
    "\n",
    "\n",
    "# Get predictions from BERT\n",
    "predictions2 = trainer3.predict(tokenized_adversarial_dataset[\"test\"]).predictions\n",
    "probs2 = torch.nn.functional.softmax(torch.tensor(predictions2), dim=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28671902 0.25789323 0.4553877 ]\n",
      " [0.2506978  0.20299646 0.5463058 ]\n",
      " [0.92324656 0.0273671  0.04938633]\n",
      " ...\n",
      " [0.06172665 0.6760523  0.26222104]\n",
      " [0.16637705 0.23884837 0.59477454]\n",
      " [0.24277243 0.5033937  0.25383392]]\n"
     ]
    }
   ],
   "source": [
    "# print(predictions1)\n",
    "print(probs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32372493 0.25584534 0.42042968]\n",
      " [0.98101085 0.00346965 0.01551953]\n",
      " [0.41557115 0.11741773 0.46701112]\n",
      " ...\n",
      " [0.96817666 0.01148273 0.02034066]\n",
      " [0.17438652 0.56347656 0.26213694]\n",
      " [0.06813377 0.3216241  0.6102421 ]]\n"
     ]
    }
   ],
   "source": [
    "print(probs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_probs = (probs1 + probs2) / 2\n",
    "ensemble_predictions = np.argmax(average_probs, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5370919881305638, 'f1': 0.5398555478694804, 'precision': 0.5553070753923801, 'recall': 0.5370919881305638}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load metrics\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "\n",
    "# Compute metrics\n",
    "# labels = tokenized_datasets_nli[\"validation\"][\"label\"]\n",
    "labels = tokenized_adversarial_dataset[\"test\"][\"label\"]\n",
    "\n",
    "accuracy = accuracy_metric.compute(predictions=ensemble_predictions, references=labels)[\"accuracy\"]\n",
    "f1 = f1_metric.compute(predictions=ensemble_predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "precision = precision_metric.compute(predictions=ensemble_predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "recall = recall_metric.compute(predictions=ensemble_predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "\n",
    "print({\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
