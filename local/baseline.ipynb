{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Parameters\n",
    "# we will use with Distil-BERT\n",
    "language_model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "### Training Argurments\n",
    "\n",
    "# this GPU should be enough for this task to handle 32 samples per batch\n",
    "batch_size = 32\n",
    "\n",
    "# optim\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.001 # we could use e.g. 0.01 in case of very low and very high amount of data for regularization\n",
    "\n",
    "# training\n",
    "epochs = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(42)\n",
    "\n",
    "nli_dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\",trust_remote_code=True)\n",
    "adversarial_set = load_dataset(\"iperbole/adversarial_fever_nli\")\n",
    "\n",
    "\n",
    "\n",
    "### METRIC DEFINITION\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels,average=\"weighted\")[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a23d09b2fe34b2b9264960e8e7cbe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8847f087d2294422b4435e70b13b44fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '150448', 'premise': \"Roman Atwood . He is best known for his vlogs , where he posts updates about his life on a daily basis . His vlogging channel , `` RomanAtwoodVlogs '' , has a total of 3.3 billion views and 11.9 million subscribers . He also has another YouTube channel called `` RomanAtwood '' , where he posts pranks .\", 'hypothesis': 'Roman Atwood is a content creator.', 'label': 0, 'wsd': {'premise': [{'index': 0, 'text': 'Roman', 'pos': 'ADJ', 'lemma': 'roman', 'bnSynsetId': 'bn:00109913a', 'wnSynsetOffset': '2921569a', 'nltkSynset': 'roman.a.01'}, {'index': 1, 'text': 'Atwood', 'pos': 'PROPN', 'lemma': 'Atwood', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 2, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 3, 'text': 'He', 'pos': 'PRON', 'lemma': 'he', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 4, 'text': 'is', 'pos': 'AUX', 'lemma': 'be', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 5, 'text': 'best', 'pos': 'ADV', 'lemma': 'well', 'bnSynsetId': 'bn:00117603r', 'wnSynsetOffset': '12779r', 'nltkSynset': 'well.r.02'}, {'index': 6, 'text': 'known', 'pos': 'VERB', 'lemma': 'know', 'bnSynsetId': 'bn:00090143v', 'wnSynsetOffset': '594337v', 'nltkSynset': 'know.v.04'}, {'index': 7, 'text': 'for', 'pos': 'ADP', 'lemma': 'for', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 8, 'text': 'his', 'pos': 'PRON', 'lemma': 'his', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 9, 'text': 'vlogs', 'pos': 'NOUN', 'lemma': 'vlog', 'bnSynsetId': 'bn:00028604n', 'wnSynsetOffset': '7007945n', 'nltkSynset': 'play.n.01'}, {'index': 10, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 11, 'text': 'where', 'pos': 'SCONJ', 'lemma': 'where', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 12, 'text': 'he', 'pos': 'PRON', 'lemma': 'he', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 13, 'text': 'posts', 'pos': 'VERB', 'lemma': 'post', 'bnSynsetId': 'bn:00091876v', 'wnSynsetOffset': '991683v', 'nltkSynset': 'post.v.02'}, {'index': 14, 'text': 'updates', 'pos': 'NOUN', 'lemma': 'update', 'bnSynsetId': 'bn:00079238n', 'wnSynsetOffset': '6643303n', 'nltkSynset': 'update.n.01'}, {'index': 15, 'text': 'about', 'pos': 'ADP', 'lemma': 'about', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 16, 'text': 'his', 'pos': 'PRON', 'lemma': 'his', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 17, 'text': 'life', 'pos': 'NOUN', 'lemma': 'life', 'bnSynsetId': 'bn:00051045n', 'wnSynsetOffset': '13963192n', 'nltkSynset': 'life.n.01'}, {'index': 18, 'text': 'on', 'pos': 'ADP', 'lemma': 'on', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 19, 'text': 'a', 'pos': 'DET', 'lemma': 'a', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 20, 'text': 'daily', 'pos': 'ADJ', 'lemma': 'daily', 'bnSynsetId': 'bn:00100875a', 'wnSynsetOffset': '1968165a', 'nltkSynset': 'daily.s.01'}, {'index': 21, 'text': 'basis', 'pos': 'NOUN', 'lemma': 'basis', 'bnSynsetId': 'bn:00008870n', 'wnSynsetOffset': '13790912n', 'nltkSynset': 'footing.n.02'}, {'index': 22, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 23, 'text': 'His', 'pos': 'PRON', 'lemma': 'his', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 24, 'text': 'vlogging', 'pos': 'VERB', 'lemma': 'vlogge', 'bnSynsetId': 'bn:00090000v', 'wnSynsetOffset': '672277v', 'nltkSynset': 'judge.v.01'}, {'index': 25, 'text': 'channel', 'pos': 'NOUN', 'lemma': 'channel', 'bnSynsetId': 'bn:00017686n', 'wnSynsetOffset': '3006398n', 'nltkSynset': 'channel.n.07'}, {'index': 26, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 27, 'text': '`', 'pos': 'PUNCT', 'lemma': '`', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 28, 'text': '`', 'pos': 'PUNCT', 'lemma': '`', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 29, 'text': 'RomanAtwoodVlogs', 'pos': 'X', 'lemma': 'romanatwoodvlogs', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 30, 'text': \"''\", 'pos': 'PUNCT', 'lemma': \"''\", 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 31, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 32, 'text': 'has', 'pos': 'AUX', 'lemma': 'have', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 33, 'text': 'a', 'pos': 'DET', 'lemma': 'a', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 34, 'text': 'total', 'pos': 'NOUN', 'lemma': 'total', 'bnSynsetId': 'bn:00002008n', 'wnSynsetOffset': '4353803n', 'nltkSynset': 'sum.n.05'}, {'index': 35, 'text': 'of', 'pos': 'ADP', 'lemma': 'of', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 36, 'text': '3.3', 'pos': 'NUM', 'lemma': '3.3', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 37, 'text': 'billion', 'pos': 'NUM', 'lemma': 'billion', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 38, 'text': 'views', 'pos': 'NOUN', 'lemma': 'view', 'bnSynsetId': 'bn:00071511n', 'wnSynsetOffset': '881649n', 'nltkSynset': 'view.n.03'}, {'index': 39, 'text': 'and', 'pos': 'CCONJ', 'lemma': 'and', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 40, 'text': '11.9', 'pos': 'NUM', 'lemma': '11.9', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 41, 'text': 'million', 'pos': 'NUM', 'lemma': 'million', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 42, 'text': 'subscribers', 'pos': 'NOUN', 'lemma': 'subscriber', 'bnSynsetId': 'bn:00066366n', 'wnSynsetOffset': '10670483n', 'nltkSynset': 'subscriber.n.02'}, {'index': 43, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 44, 'text': 'He', 'pos': 'PRON', 'lemma': 'he', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 45, 'text': 'also', 'pos': 'ADV', 'lemma': 'also', 'bnSynsetId': 'bn:00114246r', 'wnSynsetOffset': '47534r', 'nltkSynset': 'besides.r.02'}, {'index': 46, 'text': 'has', 'pos': 'AUX', 'lemma': 'have', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 47, 'text': 'another', 'pos': 'DET', 'lemma': 'another', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 48, 'text': 'YouTube', 'pos': 'PROPN', 'lemma': 'YouTube', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 49, 'text': 'channel', 'pos': 'NOUN', 'lemma': 'channel', 'bnSynsetId': 'bn:00015144n', 'wnSynsetOffset': '5250659n', 'nltkSynset': 'duct.n.01'}, {'index': 50, 'text': 'called', 'pos': 'VERB', 'lemma': 'call', 'bnSynsetId': 'bn:00084385v', 'wnSynsetOffset': '1028748v', 'nltkSynset': 'name.v.01'}, {'index': 51, 'text': '`', 'pos': 'PUNCT', 'lemma': '`', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 52, 'text': '`', 'pos': 'PUNCT', 'lemma': '`', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 53, 'text': 'RomanAtwood', 'pos': 'PROPN', 'lemma': 'RomanAtwood', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 54, 'text': \"''\", 'pos': 'PUNCT', 'lemma': \"''\", 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 55, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 56, 'text': 'where', 'pos': 'SCONJ', 'lemma': 'where', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 57, 'text': 'he', 'pos': 'PRON', 'lemma': 'he', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 58, 'text': 'posts', 'pos': 'VERB', 'lemma': 'post', 'bnSynsetId': 'bn:00091876v', 'wnSynsetOffset': '991683v', 'nltkSynset': 'post.v.02'}, {'index': 59, 'text': 'pranks', 'pos': 'NOUN', 'lemma': 'prank', 'bnSynsetId': 'bn:00004630n', 'wnSynsetOffset': '427580n', 'nltkSynset': 'antic.n.01'}, {'index': 60, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}], 'hypothesis': [{'index': 0, 'text': 'Roman', 'pos': 'PROPN', 'lemma': 'Roman', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 1, 'text': 'Atwood', 'pos': 'PROPN', 'lemma': 'Atwood', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 2, 'text': 'is', 'pos': 'AUX', 'lemma': 'be', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 3, 'text': 'a', 'pos': 'DET', 'lemma': 'a', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 4, 'text': 'content', 'pos': 'ADJ', 'lemma': 'content', 'bnSynsetId': 'bn:00100364a', 'wnSynsetOffset': '588797a', 'nltkSynset': 'contented.a.01'}, {'index': 5, 'text': 'creator', 'pos': 'NOUN', 'lemma': 'creator', 'bnSynsetId': 'bn:00023660n', 'wnSynsetOffset': '9614315n', 'nltkSynset': 'creator.n.02'}, {'index': 6, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}]}, 'srl': {'premise': {'tokens': [{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': '.'}, {'index': 3, 'rawText': 'He'}, {'index': 4, 'rawText': 'is'}, {'index': 5, 'rawText': 'best'}, {'index': 6, 'rawText': 'known'}, {'index': 7, 'rawText': 'for'}, {'index': 8, 'rawText': 'his'}, {'index': 9, 'rawText': 'vlogs'}, {'index': 10, 'rawText': ','}, {'index': 11, 'rawText': 'where'}, {'index': 12, 'rawText': 'he'}, {'index': 13, 'rawText': 'posts'}, {'index': 14, 'rawText': 'updates'}, {'index': 15, 'rawText': 'about'}, {'index': 16, 'rawText': 'his'}, {'index': 17, 'rawText': 'life'}, {'index': 18, 'rawText': 'on'}, {'index': 19, 'rawText': 'a'}, {'index': 20, 'rawText': 'daily'}, {'index': 21, 'rawText': 'basis'}, {'index': 22, 'rawText': '.'}, {'index': 23, 'rawText': 'His'}, {'index': 24, 'rawText': 'vlogging'}, {'index': 25, 'rawText': 'channel'}, {'index': 26, 'rawText': ','}, {'index': 27, 'rawText': '`'}, {'index': 28, 'rawText': '`'}, {'index': 29, 'rawText': 'RomanAtwoodVlogs'}, {'index': 30, 'rawText': \"''\"}, {'index': 31, 'rawText': ','}, {'index': 32, 'rawText': 'has'}, {'index': 33, 'rawText': 'a'}, {'index': 34, 'rawText': 'total'}, {'index': 35, 'rawText': 'of'}, {'index': 36, 'rawText': '3.3'}, {'index': 37, 'rawText': 'billion'}, {'index': 38, 'rawText': 'views'}, {'index': 39, 'rawText': 'and'}, {'index': 40, 'rawText': '11.9'}, {'index': 41, 'rawText': 'million'}, {'index': 42, 'rawText': 'subscribers'}, {'index': 43, 'rawText': '.'}, {'index': 44, 'rawText': 'He'}, {'index': 45, 'rawText': 'also'}, {'index': 46, 'rawText': 'has'}, {'index': 47, 'rawText': 'another'}, {'index': 48, 'rawText': 'YouTube'}, {'index': 49, 'rawText': 'channel'}, {'index': 50, 'rawText': 'called'}, {'index': 51, 'rawText': '`'}, {'index': 52, 'rawText': '`'}, {'index': 53, 'rawText': 'RomanAtwood'}, {'index': 54, 'rawText': \"''\"}, {'index': 55, 'rawText': ','}, {'index': 56, 'rawText': 'where'}, {'index': 57, 'rawText': 'he'}, {'index': 58, 'rawText': 'posts'}, {'index': 59, 'rawText': 'pranks'}, {'index': 60, 'rawText': '.'}], 'annotations': [{'tokenIndex': 4, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 22]}]}, 'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARG2', 'score': 1.0, 'span': [5, 22]}]}}, {'tokenIndex': 6, 'verbatlas': {'frameName': 'KNOW', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 6]}, {'role': 'Topic', 'score': 1.0, 'span': [7, 22]}]}, 'englishPropbank': {'frameName': 'know.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARGM-MNR', 'score': 1.0, 'span': [5, 6]}, {'role': 'ARG2', 'score': 1.0, 'span': [7, 22]}]}}, {'tokenIndex': 13, 'verbatlas': {'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [8, 10]}, {'role': 'Agent', 'score': 1.0, 'span': [12, 13]}, {'role': 'Theme', 'score': 1.0, 'span': [14, 18]}, {'role': 'Time', 'score': 1.0, 'span': [18, 22]}]}, 'englishPropbank': {'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [8, 10]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [11, 12]}, {'role': 'ARG0', 'score': 1.0, 'span': [12, 13]}, {'role': 'ARG1', 'score': 1.0, 'span': [14, 18]}, {'role': 'ARGM-TMP', 'score': 1.0, 'span': [18, 22]}]}}, {'tokenIndex': 32, 'verbatlas': {'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [23, 32]}, {'role': 'Attribute', 'score': 1.0, 'span': [33, 43]}]}, 'englishPropbank': {'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [23, 32]}, {'role': 'ARG1', 'score': 1.0, 'span': [33, 43]}]}}, {'tokenIndex': 46, 'verbatlas': {'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [44, 45]}, {'role': 'Attribute', 'score': 1.0, 'span': [47, 60]}]}, 'englishPropbank': {'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [44, 45]}, {'role': 'ARGM-DIS', 'score': 1.0, 'span': [45, 46]}, {'role': 'ARG1', 'score': 1.0, 'span': [47, 60]}]}}, {'tokenIndex': 50, 'verbatlas': {'frameName': 'NAME', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [47, 50]}, {'role': 'Attribute', 'score': 1.0, 'span': [51, 55]}]}, 'englishPropbank': {'frameName': 'call.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [47, 50]}, {'role': 'ARG2', 'score': 1.0, 'span': [51, 55]}]}}, {'tokenIndex': 58, 'verbatlas': {'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [47, 55]}, {'role': 'Agent', 'score': 1.0, 'span': [57, 58]}, {'role': 'Theme', 'score': 1.0, 'span': [59, 60]}]}, 'englishPropbank': {'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [47, 55]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [56, 57]}, {'role': 'ARG0', 'score': 1.0, 'span': [57, 58]}, {'role': 'ARG1', 'score': 1.0, 'span': [59, 60]}]}}]}, 'hypothesis': {'tokens': [{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': 'is'}, {'index': 3, 'rawText': 'a'}, {'index': 4, 'rawText': 'content'}, {'index': 5, 'rawText': 'creator'}, {'index': 6, 'rawText': '.'}], 'annotations': [{'tokenIndex': 2, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]}, {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]}, 'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [0, 2]}, {'role': 'ARG2', 'score': 1.0, 'span': [3, 6]}]}}]}}, 'input_ids': [101, 3142, 2012, 3702, 1012, 2002, 2003, 2190, 2124, 2005, 2010, 1058, 21197, 2015, 1010, 2073, 2002, 8466, 14409, 2055, 2010, 2166, 2006, 1037, 3679, 3978, 1012, 2010, 1058, 21197, 4726, 3149, 1010, 1036, 1036, 3142, 4017, 3702, 2615, 21197, 2015, 1005, 1005, 1010, 2038, 1037, 2561, 1997, 1017, 1012, 1017, 4551, 5328, 1998, 2340, 1012, 1023, 2454, 17073, 1012, 2002, 2036, 2038, 2178, 7858, 3149, 2170, 1036, 1036, 3142, 4017, 3702, 1005, 1005, 1010, 2073, 2002, 8466, 26418, 2015, 1012, 102, 3142, 2012, 3702, 2003, 1037, 4180, 8543, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "## Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(language_model_name,\n",
    "                                                                   ignore_mismatched_sizes=True,\n",
    "                                                                   output_attentions=False, output_hidden_states=False,\n",
    "                                                                   num_labels=3) # number of the classes to change to 3\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "\n",
    "# padding with the most long sentence!\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # avoid to use can reduce the memory on GPU\n",
    "\n",
    "#examples are batch!\n",
    "def tokenize_function(examples):\n",
    "    examples[\"label\"] = [labels_mapping[label] for label in examples[\"label\"]]\n",
    "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"],padding = True, truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the dataset ...\n",
    "print(\"Tokenize the dataset ...\")\n",
    "labels_mapping = {\"ENTAILMENT\":0, \"CONTRADICTION\":1, \"NEUTRAL\":2 }\n",
    "tokenized_datasets_nli = nli_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_adversarial_dataset = adversarial_set.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_datasets_nli[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effb0f7ed410460bbbb357a461d1df86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bitfra/miniconda3/envs/nlp_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6619, 'grad_norm': 3.862337350845337, 'learning_rate': 0.0001, 'epoch': 0.31}\n",
      "{'loss': 0.4778, 'grad_norm': 3.6819708347320557, 'learning_rate': 5.44211485870556e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4284, 'grad_norm': 7.138038158416748, 'learning_rate': 8.842297174111212e-06, 'epoch': 0.94}\n",
      "{'train_runtime': 1595.9478, 'train_samples_per_second': 32.01, 'train_steps_per_second': 1.001, 'train_loss': 0.5150247715978079, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1597, training_loss=0.5150247715978079, metrics={'train_runtime': 1595.9478, 'train_samples_per_second': 32.01, 'train_steps_per_second': 1.001, 'total_flos': 6753498842284992.0, 'train_loss': 0.5150247715978079, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#MODEL TRAINING\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_dir\",                    # output directory [Mandatory]\n",
    "    num_train_epochs=epochs,                      # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,       # batch size per device during training\n",
    "    warmup_steps=500,                             # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=weight_decay,                    # strength of weight decay\n",
    "    save_strategy=\"no\",                           # save the model\n",
    "    learning_rate=learning_rate,                  # learning rate\n",
    "    gradient_checkpointing = True                 # to reduce memory usage\n",
    "    # fp16 = True                                 # to reduce more memory usage\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_datasets_nli[\"train\"],\n",
    "   eval_dataset=tokenized_datasets_nli[\"validation\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337fae060cd44760a7cbf702ce964ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b141fba1f6244c1a9c5e6f3b690110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validatioon => {'eval_loss': 0.7188450694084167, 'eval_accuracy': 0.7355769230769231, 'eval_f1': 0.7272669394266372, 'eval_runtime': 12.4568, 'eval_samples_per_second': 183.674, 'eval_steps_per_second': 22.959, 'epoch': 1.0}\n",
      "test => {'eval_loss': 0.7704156637191772, 'eval_accuracy': 0.7039790118058592, 'eval_f1': 0.6946643382506382, 'eval_runtime': 12.0497, 'eval_samples_per_second': 189.798, 'eval_steps_per_second': 23.735, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_results = trainer.evaluate()\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_datasets_nli['test'])\n",
    "\n",
    "print(f\"validatioon => {validation_results}\")\n",
    "print(f\"test => {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a894974f57b2498ebe41ec8c83a7a2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial test =>  {'eval_loss': 1.354268193244934, 'eval_accuracy': 0.49258160237388726, 'eval_f1': 0.4937813379136295, 'eval_runtime': 2.084, 'eval_samples_per_second': 161.705, 'eval_steps_per_second': 20.633, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "test_augmented_result = trainer.evaluate(tokenized_adversarial_dataset['test'])\n",
    "print(\"adversarial test => \",test_augmented_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
